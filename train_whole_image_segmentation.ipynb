{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dos\n",
    "# * up_mode\n",
    "# * hyperparametes: depth, wf, edge_weight, max/avd pool\n",
    "# * train on whole training set\n",
    "# * metrics: loss functions for image segmentation, Jaccard index\n",
    "# * twitch model: hyperparameter tuning\n",
    "# * more training (increase num_epochs)\n",
    "# * important parameters: depth, edge_weight, \n",
    "# * should the transformations be the same for viable and whole tasks\n",
    "# * change optimizer: read article on optimizers http://ruder.io/optimizing-gradient-descent/\n",
    "# * verify loss at init\n",
    "# * overfit a tiny subset of the data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"paip2019\"\n",
    "gpu_id = 0\n",
    "\n",
    "\n",
    "############################\n",
    "# PREPROCESSING PARAMETERS #\n",
    "############################\n",
    "num_classes = 2\n",
    "in_channels = 3\n",
    "padding = True\n",
    "depth = 5\n",
    "wf = 2\n",
    "up_mode = 'upconv'\n",
    "batch_norm = True\n",
    "\n",
    "\n",
    "#######################\n",
    "# TRAINING PARAMETERS #\n",
    "#######################\n",
    "batch_size = 5\n",
    "patch_size = 256\n",
    "num_epochs = 100\n",
    "ignore_index = -100\n",
    "\n",
    "# edges tend to be the most poorly segmented given how little area \n",
    "# they occupy in the training set, this paramter boosts their values \n",
    "# along the lines of the original UNET paper; test with different values\n",
    "edge_weight = 2.0 #*\n",
    "stages = [\"train\", \"val\"]\n",
    "validation_stages = [\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from unet import UNet\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys, glob\n",
    "\n",
    "from network import R2AttU_Net\n",
    "from models import UNet16\n",
    "\n",
    "import scipy.ndimage\n",
    "\n",
    "import time\n",
    "import math\n",
    "import tables\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pretty printing of current time and remaining time\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent+.00001)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_properties(gpu_id))\n",
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model according to the specified parameters and copy it to the GPU\n",
    "# model = UNet(n_classes=num_classes, in_channels=in_channels, padding=padding, depth=depth, wf=wf,\n",
    "#             up_mode=up_mode, batch_norm=batch_norm).to(device)\n",
    "\n",
    "model = UNet16(num_classes=num_classes, num_filters=8, pretrained=True).to(device)\n",
    "\n",
    "#print(model)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WholeDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Class which defines the data model for the whole mask segmentation task.\n",
    "        \n",
    "        Attributes:\n",
    "         - file_name (string) : path to the pytable containing the data\n",
    "         - image_transform (callable, optional): optional transform to be applied on the WSI data\n",
    "         - mask_transform (callable, optional): optional transform to be applied on the mask data \n",
    "         - edge_weight (float):\n",
    "         - tables (pytable object): the db object which stores the training/validation data\n",
    "         - num_items (int): the number of data samples in the dataset\n",
    "         - num_pixels (int): the number of pixels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_name, image_transform=None, mask_transform=None, edge_weight=False):\n",
    "        self.file_name = file_name\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        self.tables = tables.open_file(self.file_name)\n",
    "        self.num_items = self.tables.root.wsi.shape[0]\n",
    "        self.wnumpixels = self.tables.root.wnumpixels[:]\n",
    "        self.tables.close()\n",
    "        \n",
    "        self.images = None\n",
    "        self.whole_masks = None\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        with tables.open_file(self.file_name, 'r') as db:\n",
    "            # obtain the wsi-viable_mask pairs for the requested batch\n",
    "            image = db.root.wsi[index,:,:,:]\n",
    "            whole_mask = db.root.whole[index,:,:]\n",
    "           \n",
    "            # the original paper assigns increased weights to the edges of the annotation\n",
    "            # use faster method: simply dilate and highlight all the pixels which were added            \n",
    "            if (self.edge_weight):\n",
    "                whole_weight = scipy.ndimage.morphology.binary_dilation(whole_mask==1,iterations=2) & ~whole_mask\n",
    "            else: # otherwise the edge weight is all ones and thus no effect on learning\n",
    "                whole_weight = np.ones(whole_mask.shape, dtype=whole_mask.dtype)\n",
    "            \n",
    "            # reshape in order to use transformations from torchvision\n",
    "            whole_mask = whole_mask[:,:,None].repeat(3, axis=2)\n",
    "            whole_weight = whole_weight[:,:,None].repeat(3, axis=2)\n",
    "            \n",
    "            # get random seed so that the transformations are reproducible\n",
    "            seed = random.randrange(sys.maxsize)\n",
    "            \n",
    "            if self.image_transform is not None:\n",
    "                random.seed(seed)\n",
    "                image_new = self.image_transform(image)\n",
    "            \n",
    "            if self.mask_transform is not None:\n",
    "                random.seed(seed)                \n",
    "                whole_mask_new = self.mask_transform(whole_mask)\n",
    "                whole_mask_new = np.asarray(whole_mask_new)[:,:,0].squeeze()                \n",
    "                \n",
    "                random.seed(seed)\n",
    "                whole_weight_new = self.mask_transform(whole_weight)\n",
    "                whole_weight_new = np.asarray(whole_weight_new)[:,:,0].squeeze()        \n",
    "                \n",
    "        return image_new, whole_mask_new, whole_weight_new \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that since we need the transofrmations to be reproducible for both masks and images\n",
    "#we do the spatial transformations first, and afterwards do any color augmentations\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=(patch_size, patch_size),pad_if_needed=True),\n",
    "        transforms.RandomResizedCrop(size=patch_size),\n",
    "        transforms.RandomRotation(180),\n",
    "        transforms.ColorJitter(brightness=0,contrast=0,saturation=0,hue=.5),\n",
    "        transforms.RandomGrayscale(),\n",
    "        transforms.ToTensor() ])\n",
    "\n",
    "# try different transformation for viable and whole mask respectively\n",
    "mask_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True),\n",
    "        transforms.RandomResizedCrop(size=patch_size,interpolation=PIL.Image.NEAREST),\n",
    "        transforms.RandomRotation(180) ])\n",
    "#    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "data_loader = {}\n",
    "\n",
    "# create the data loader for both training and validation stages\n",
    "for stage in stages:\n",
    "    dataset[stage] = WholeDataset(f'./pytables/{data_name}_{stage}.pytable', image_transform=image_transform, mask_transform=mask_transform, edge_weight=edge_weight)\n",
    "    data_loader[stage] = DataLoader(dataset[stage],batch_size=batch_size,shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize a single example to verify that it is correct\n",
    "idx = 677\n",
    "(img, whole_mask, whole_mask_weight)=dataset[\"train\"][idx]\n",
    "print(img.shape)\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,5))  # 1 row, 2 columns\n",
    "\n",
    "#build output showing original patch  (after augmentation), class = 1 mask, weighting mask, overall mask (to see any ignored classes)\n",
    "ax[0].set_title(\"original wsi\")\n",
    "ax[0].imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "\n",
    "ax[1].set_title(\"whole mask\")\n",
    "ax[1].imshow(whole_mask)\n",
    "\n",
    "ax[2].set_title(\"whole edge weight\")\n",
    "ax[2].imshow(whole_mask_weight)\n",
    "\n",
    "plt.savefig(f'D:\\work2019-2020\\PAIP_2019\\data\\Patches\\whole_patch{idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "# Adam is the most robust but for better performance use SGD\n",
    "# test with different optimizers; study optimizers\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "#optim = torch.optim.SGD(model.parameters(), lr=.1, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight individual classes based on their presence in the training set to avoid biasing any particular class\n",
    "nclasses = dataset[\"train\"].wnumpixels.shape[1]\n",
    "print(nclasses)\n",
    "\n",
    "whole_class_weight = dataset[\"train\"].wnumpixels[1,0:2]\n",
    "print(whole_class_weight)\n",
    "whole_class_weight = torch.from_numpy(1-whole_class_weight/whole_class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "print(f'whole class weight: {whole_class_weight}')\n",
    "\n",
    "#reduce = False makes sure we get a 2D output instead of a 1D \"summary\" value\n",
    "criterion = nn.CrossEntropyLoss(weight=whole_class_weight, ignore_index=ignore_index, reduction='none') \n",
    "#criterion = LossBinary(viable_class_weight, jaccard_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial settings\n",
    "best_loss_on_test = np.Infinity\n",
    "edge_weight = torch.tensor(edge_weight).to(device)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # initialize epoch-level performance variables\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = {key: torch.zeros(0).to(device) for key in stages}\n",
    "    cmatrix = np.zeros((2,2))\n",
    "    #jaccard = torch.zeros(0).to(device) # jaccard index to make an idea of overral performance\n",
    "    \n",
    "    for stage in stages:\n",
    "        if stage == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for batch_idx, (X, y, y_weight) in enumerate(data_loader[stage]):            \n",
    "            X = X.to(device) # [Nbatch, 3, H, W]\n",
    "            y_weight = y_weight.type('torch.FloatTensor').to(device) # [NBatch, H, W]\n",
    "            y = y.type('torch.LongTensor').to(device) # [NBatch, H, W] with class indexes (0,1)\n",
    "        \n",
    "            with torch.set_grad_enabled(stage=='train'):\n",
    "                prediction = model(X) # [NBatch, Nclass, H, W]\n",
    "                loss_matrix = criterion(prediction, y)\n",
    "                loss = (loss_matrix * (edge_weight ** y_weight)).mean()\n",
    "                \n",
    "                # backpropagation\n",
    "                if stage == \"train\":\n",
    "                    optim.zero_grad() # clear previous gradients\n",
    "                    loss.backward() # compute gradients of all variables wrt to loss\n",
    "                    optim.step() # perform updates using calculated gradients\n",
    "                \n",
    "                epoch_loss[stage] = torch.cat((epoch_loss[stage], loss.detach().view(1,-1)))\n",
    "                \n",
    "                # if this phase is part of validation, compute confusion matrix\n",
    "                if stage in validation_stages:\n",
    "                    p = prediction[:,:,:,:].detach().cpu().numpy()\n",
    "                    class_pred = np.argmax(p, axis=1).flatten()\n",
    "                    mask = y.cpu().flatten()                    \n",
    "                    cmatrix = cmatrix + confusion_matrix(mask, class_pred, labels=range(num_classes))\n",
    "#                     if not batch_idx % 50:\n",
    "#                         print(f'jaccard index: {jaccard_index(cmatrix)}')\n",
    "                    #jaccard = torch.cat((jaccard, jaccard_index(cmatrix).view(1,-1)))\n",
    "        \n",
    "        epoch_acc = (cmatrix / cmatrix.sum()).trace()\n",
    "        epoch_loss[stage] = epoch_loss[stage].cpu().numpy().mean()\n",
    "        #jaccard = jaccard.cpu().numpy().mean()\n",
    "\n",
    "    print('%s ([%d/%d] %d%%), train loss: %.4f test loss: %.4f accuracy: %.4f%%' \\\n",
    "          %(timeSince(start_time, (epoch+1) / num_epochs), epoch+1, num_epochs, (epoch+1) / num_epochs * 100, \\\n",
    "            epoch_loss[\"train\"], epoch_loss[\"val\"], epoch_acc*100), end=\"\")    \n",
    "\n",
    "    #if current loss is the best we've seen, save model state with all variables\n",
    "    if epoch_loss[\"val\"] < best_loss_on_test:\n",
    "        best_loss_on_test = epoch_loss[\"val\"]\n",
    "        print(\"  **\")\n",
    "        state = {'epoch': epoch + 1,\n",
    "                 'model_dict': model.state_dict(),\n",
    "                 'optim_dict': optim.state_dict(),\n",
    "                 'best_loss_on_test': epoch_loss,\n",
    "                 'n_classes': num_classes,\n",
    "                 'in_channels': in_channels,\n",
    "                 'padding': padding,\n",
    "                 'depth': depth,\n",
    "                 'wf': wf,\n",
    "                 'up_mode': up_mode, \n",
    "                 'batch_norm': batch_norm\n",
    "                }\n",
    "\n",
    "        torch.save(state, f\"{data_name}_unet_whole.pth\")   # best loss 0.1030\n",
    "    else:\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
